import os
import sys

# Import Streamlit first to avoid conflicts
try:
    import streamlit as st
    st.write("üöÄ Starting Emotion Recognition App...")
    st.write("‚úÖ Streamlit imported successfully")
except ImportError as e:
    print(f"‚ùå Failed to import Streamlit: {e}")
    sys.exit(1)

# Set environment variables before any imports to handle Keras compatibility
os.environ['TF_USE_LEGACY_KERAS'] = '1'
os.environ['TF_KERAS'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['PYTHONHASHSEED'] = '0'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU usage to avoid GPU memory issues

# Memory management for stability
import gc
gc.set_threshold(700, 10, 10)

# Add error handling for imports
print("üîÑ Starting application...")
print(f"Python version: {sys.version}")

try:
    print("ü§ñ Importing DeepFace...")
    from deepface import DeepFace
    print("‚úÖ DeepFace imported successfully")
    
    print("üìä Importing matplotlib...")
    import matplotlib.pyplot as plt
    plt.switch_backend('Agg')  # Use non-interactive backend for stability
    print("‚úÖ matplotlib imported successfully")
    
    print("üëÅÔ∏è Importing OpenCV...")
    import cv2
    print("‚úÖ OpenCV imported successfully")
    
    print("üî¢ Importing numpy...")
    import numpy as np
    print("‚úÖ numpy imported successfully")
    import tempfile
    from PIL import Image, ImageDraw, ImageFont
    import matplotlib.patches as patches
    
    # Import with type stubs for optional webrtc
    try:
        from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration  # type: ignore
        import av
        WEBRTC_AVAILABLE = True
        
        # Define the real VideoProcessor when webrtc is available
        class VideoProcessor(VideoTransformerBase):  # type: ignore
            """Klasa do przetwarzania wideo z kamery w czasie rzeczywistym"""
            
            def __init__(self):
                self.frame_count = 0
                self.analyze_every_n_frames = 30  # Analizuj co 30 klatek (oko≈Ço sekundy przy 30 FPS)
            
            def recv(self, frame):
                global latest_emotion_result, emotion_lock
                
                img = frame.to_ndarray(format="bgr24")
                
                # Analizuj emocje co N klatek ≈ºeby nie obciƒÖ≈ºaƒá procesora
                if self.frame_count % self.analyze_every_n_frames == 0:
                    try:
                        # Zapisz klatkƒô tymczasowo
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                            cv2.imwrite(tmp_file.name, img)
                            
                            # Analizuj emocje (szybko, bez enforce_detection) with error handling
                            try:
                                result = DeepFace.analyze(
                                    tmp_file.name, 
                                    actions=['emotion'], 
                                    enforce_detection=False,
                                    silent=True,
                                    detector_backend='opencv'  # Use stable backend
                                )
                                
                                # Zapisz wynik
                                with emotion_lock:
                                    if isinstance(result, list):
                                        latest_emotion_result = result[0]
                                    else:
                                        latest_emotion_result = result
                            except Exception:
                                # Silently handle analysis errors in real-time mode
                                pass
                            
                            # Usu≈Ñ plik tymczasowy
                            os.unlink(tmp_file.name)
                            
                    except Exception as e:
                        pass  # Zignoruj b≈Çƒôdy analizy
                
                # Je≈õli mamy wynik analizy, narysuj na obrazie
                with emotion_lock:
                    if latest_emotion_result is not None:
                        try:
                            emotions = latest_emotion_result.get('emotion', {})  # type: ignore
                            face_region = latest_emotion_result.get('region', {})  # type: ignore
                            
                            if emotions and face_region:
                                # Znajd≈∫ dominujƒÖcƒÖ emocjƒô
                                dominant_emotion = max(emotions.items(), key=lambda x: x[1])
                                
                                # Narysuj prostokƒÖt wok√≥≈Ç twarzy
                                x, y, w, h = face_region.get('x', 0), face_region.get('y', 0), face_region.get('w', 0), face_region.get('h', 0)
                                if x > 0 and y > 0 and w > 0 and h > 0:
                                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
                                    
                                    # Dodaj tekst z emocjƒÖ
                                    emotion_emoji = {
                                        'happy': 'üòä', 'sad': 'üò¢', 'angry': 'üò†', 'surprise': 'üòÆ', 
                                        'fear': 'üò®', 'disgust': 'ü§¢', 'neutral': 'üòê'
                                    }
                                    emoji = emotion_emoji.get(dominant_emotion[0], 'üé≠')
                                    label = f"{dominant_emotion[0]}: {dominant_emotion[1]:.1f}%"  # UsunƒÖ≈Çem emoji bo mo≈ºe nie dzia≈Çaƒá w OpenCV
                                    
                                    # Rysuj t≈Ço dla tekstu
                                    font = cv2.FONT_HERSHEY_SIMPLEX
                                    font_scale = 0.6
                                    thickness = 2
                                    (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
                                    
                                    cv2.rectangle(img, (x, y - text_height - 10), (x + text_width, y), (0, 255, 0), -1)
                                    cv2.putText(img, label, (x, y - 5), font, font_scale, (0, 0, 0), thickness)
                            
                        except Exception as e:
                            pass  # Zignoruj b≈Çƒôdy rysowania
                
                self.frame_count += 1
                return av.VideoFrame.from_ndarray(img, format="bgr24")
        
    except ImportError:
        WEBRTC_AVAILABLE = False
        st.warning("‚ö†Ô∏è Camera functionality is not available. Only file upload mode will work.")
        
        # Define dummy classes when webrtc is not available
        class VideoTransformerBase:
            pass
            
        class VideoProcessor:
            def __init__(self):
                self.frame_count = 0
                self.analyze_every_n_frames = 30
                
        class RTCConfiguration:
            def __init__(self, *args, **kwargs):
                pass
                
        def webrtc_streamer(*args, **kwargs):
            return None
    
    import threading
    import time
    from typing import Optional, Dict, Any, Tuple
except ImportError as e:
    print(f"Import error: {e}")
    sys.exit(1)

# Memory management functions
def cleanup_memory():
    """Clean up memory to prevent segmentation faults"""
    gc.collect()
    plt.close('all')

@st.cache_data(show_spinner=False)
def load_deepface_model():
    """Preload DeepFace model with caching"""
    try:
        # Trigger model loading with a dummy analysis
        import numpy as np
        dummy_img = np.zeros((224, 224, 3), dtype=np.uint8)
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
            cv2.imwrite(tmp_file.name, dummy_img)
            try:
                DeepFace.analyze(tmp_file.name, actions=['emotion'], enforce_detection=False, silent=True)
            except:
                pass
            finally:
                os.unlink(tmp_file.name)
        return True
    except Exception:
        return False

# Konfiguracja strony
st.set_page_config(
    page_title="üé≠ Analizator Emocji AI",
    page_icon="üé≠",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Stylizacja CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .sub-header {
        font-size: 1.5rem;
        color: #ff7f0e;
        margin: 1rem 0;
        border-left: 4px solid #ff7f0e;
        padding-left: 1rem;
    }
    .metric-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin: 0.5rem 0;
    }
    .emotion-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .sidebar-content {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    div.stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 0.5rem 2rem;
        font-weight: bold;
        transition: all 0.3s;
    }
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .uploadedFile {
        border: 2px dashed #1f77b4;
        border-radius: 10px;
        padding: 1rem;
        text-align: center;
        background: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)

def draw_emotion_on_face(image_path: str, face_region: Dict[str, int], dominant_emotion: str, confidence: float) -> Optional[np.ndarray]:
    """Rysuje prostokƒÖt wok√≥≈Ç twarzy i oznacza emocjƒô"""
    # Wczytaj obraz
    img = cv2.imread(image_path)
    if img is None:
        return None
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # Pobierz wsp√≥≈Çrzƒôdne twarzy
    x, y, w, h = face_region['x'], face_region['y'], face_region['w'], face_region['h']
    
    # Narysuj prostokƒÖt wok√≥≈Ç twarzy
    cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 0), 3)
    
    # Dodaj tekst z emocjƒÖ
    label = f"{dominant_emotion}: {confidence:.1f}%"
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.8
    thickness = 2
    
    # Oblicz rozmiar tekstu
    (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
    
    # Narysuj t≈Ço dla tekstu
    cv2.rectangle(img_rgb, (x, y - text_height - 10), (x + text_width, y), (0, 255, 0), -1)
    
    # Narysuj tekst
    cv2.putText(img_rgb, label, (x, y - 5), font, font_scale, (0, 0, 0), thickness)
    
    return img_rgb

def create_face_analysis_plot(image_path: str, result: Any) -> Tuple[Optional[np.ndarray], Optional[Dict[str, float]], Optional[Tuple[str, float]]]:
    """Tworzy wykres z zaznaczonƒÖ twarzƒÖ i emocjami"""
    # Wczytaj obraz
    img = cv2.imread(image_path)
    if img is None:
        return None, None, None
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # Przygotuj dane
    if isinstance(result, list):
        face_data = result[0]
    else:
        face_data = result
    
    emotions = face_data['emotion']
    face_region = face_data['region']
    dominant_emotion = max(emotions.items(), key=lambda x: x[1])
    
    # Narysuj obraz z oznaczonƒÖ twarzƒÖ
    annotated_img = draw_emotion_on_face(image_path, face_region, dominant_emotion[0], dominant_emotion[1])
    
    return annotated_img, emotions, dominant_emotion

# Globalne zmienne dla kamery
latest_emotion_result = None
emotion_lock = threading.Lock()

# G≈Ç√≥wny nag≈Ç√≥wek aplikacji
st.markdown('<h1 class="main-header">üé≠ Analizator Emocji AI</h1>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Wykrywaj emocje na zdjƒôciach dziƒôki sztucznej inteligencji</p>', unsafe_allow_html=True)

# === SIDEBAR ===
st.sidebar.markdown("## ÔøΩÔ∏è Panel Kontrolny")

# Informacje o aplikacji
with st.sidebar.expander("‚ÑπÔ∏è O Aplikacji", expanded=True):
    st.markdown("""
    **ÔøΩüé≠ Analizator Emocji AI** u≈ºywa zaawansowanych algorytm√≥w deep learning do:
    
    ‚Ä¢ üîç Wykrywania twarzy na zdjƒôciach
    ‚Ä¢ üéØ Analizy wyrazu emocjonalnego
    ‚Ä¢ üìä Generowania szczeg√≥≈Çowych raport√≥w
    ‚Ä¢ üé® Wizualizacji wynik√≥w
    """)

# Ustawienia analizy
st.sidebar.markdown("### ‚öôÔ∏è Ustawienia Analizy")

# Wyb√≥r ≈∫r√≥d≈Ça obrazu
if WEBRTC_AVAILABLE:
    source_options = ["üì∏ Przesy≈Çanie pliku", "üìπ Kamera internetowa"]
else:
    source_options = ["üì∏ Przesy≈Çanie pliku"]
    
source_option = st.sidebar.radio(
    "üìπ ≈πr√≥d≈Ço obrazu:",
    source_options,
    help="Wybierz skƒÖd chcesz analizowaƒá emocje"
)

confidence_threshold = st.sidebar.slider(
    "üéØ Pr√≥g pewno≈õci (%)", 
    min_value=0, 
    max_value=100, 
    value=50,
    help="Minimalny poziom pewno≈õci dla wykrywania twarzy"
)

show_advanced = st.sidebar.checkbox("üî¨ Poka≈º zaawansowane opcje", False)

if show_advanced:
    detection_backend = st.sidebar.selectbox(
        "üîç Backend wykrywania",
        ["opencv", "retinaface", "mtcnn"],
        index=0,
        help="Wybierz algorytm wykrywania twarzy"
    )
    
    model_name = st.sidebar.selectbox(
        "üß† Model analizy",
        ["VGG-Face", "Facenet", "OpenFace", "DeepFace"],
        index=0,
        help="Wybierz model do analizy emocji"
    )
else:
    detection_backend = "opencv"
    model_name = "VGG-Face"

# Statystyki
st.sidebar.markdown("### üìà Statystyki")
with st.sidebar.container():
    st.markdown("""
    <div class="metric-container">
        <h4>üéØ Emocje do wykrycia:</h4>
        <p>üòä Rado≈õƒá ‚Ä¢ üò¢ Smutek ‚Ä¢ üò† Z≈Ço≈õƒá<br>
        üòÆ Zdziwienie ‚Ä¢ üò® Strach ‚Ä¢ ü§¢ Obrzydzenie<br>
        üòê Neutralno≈õƒá</p>
    </div>
    """, unsafe_allow_html=True)

# Pomoc
with st.sidebar.expander("üí° Wskaz√≥wki"):
    st.markdown("""
    **üì∏ Najlepsze rezultaty osiƒÖgniesz gdy:**
    
    ‚úÖ Twarz jest dobrze o≈õwietlona  
    ‚úÖ Osoba patrzy w kierunku kamery  
    ‚úÖ Zdjƒôcie jest ostre i wyra≈∫ne  
    ‚úÖ Twarz zajmuje wiƒôkszƒÖ czƒô≈õƒá kadru  
    
    **‚ùå Unikaj:**
    
    ‚ùå Zdjƒôƒá zbyt ciemnych lub jasnych  
    ‚ùå Twarzy zakrytych lub w profilu  
    ‚ùå Zdjƒôƒá rozmytych lub pixelowanych  
    """)

# === G≈Å√ìWNA ZAWARTO≈öƒÜ ===

if source_option == "üì∏ Przesy≈Çanie pliku":
    # Sekcja upload-u zdjƒôcia
    st.markdown('<div class="sub-header">üì∏ Prze≈õlij Zdjƒôcie do Analizy</div>', unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        uploaded_file = st.file_uploader(
            "",
            type=["jpg", "jpeg", "png", "bmp", "tiff"],
            help="Wybierz zdjƒôcie z wyra≈∫nie widocznƒÖ twarzƒÖ",
            label_visibility="collapsed"
        )
        
        if uploaded_file is None:
            st.markdown("""
            <div class="uploadedFile">
                <h3>üì∏ PrzeciƒÖgnij i upu≈õƒá zdjƒôcie tutaj</h3>
                <p>lub kliknij aby wybraƒá plik</p>
                <p><small>Obs≈Çugiwane formaty: JPG, PNG, BMP, TIFF</small></p>
            </div>
            """, unsafe_allow_html=True)

else:  # Kamera internetowa
    if WEBRTC_AVAILABLE:
        st.markdown('<div class="sub-header">üìπ Kamera Internetowa - Analiza Real-time</div>', unsafe_allow_html=True)
        
        st.markdown("""
        <div class="emotion-card">
            <h4>üé• Analiza emocji w czasie rzeczywistym</h4>
            <p>‚Ä¢ Kamera analizuje Twoje emocje na ≈ºywo</p>
            <p>‚Ä¢ Wyniki sƒÖ aktualizowane co oko≈Ço sekundƒô</p>
            <p>‚Ä¢ Zielony prostokƒÖt pokazuje wykrytƒÖ twarz</p>
            <p>‚Ä¢ Nazwa emocji pojawia siƒô nad twarzƒÖ</p>
        </div>
        """, unsafe_allow_html=True)
        
        if WEBRTC_AVAILABLE:
            # Konfiguracja WebRTC
            RTC_CONFIGURATION = RTCConfiguration({
                "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
            })
            
            # Stream z kamery z analizƒÖ emocji
            webrtc_ctx = webrtc_streamer(
                key="emotion-analysis",
                video_processor_factory=VideoProcessor,
                rtc_configuration=RTC_CONFIGURATION,
                media_stream_constraints={"video": True, "audio": False},
                async_processing=True,
            )
            
            # Wy≈õwietlaj bie≈ºƒÖce wyniki analizy
            if webrtc_ctx and webrtc_ctx.video_processor:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### üìä Bie≈ºƒÖca Analiza")
                    emotion_placeholder = st.empty()
                    
                with col2:
                    st.markdown("#### üéØ Statystyki")
                    stats_placeholder = st.empty()
                
                # Aktualizuj wyniki w czasie rzeczywistym
                if hasattr(webrtc_ctx, 'state') and webrtc_ctx.state.playing:
                    with emotion_lock:
                        if latest_emotion_result is not None:
                            try:
                                emotions = latest_emotion_result.get('emotion', {})  # type: ignore
                                if emotions:
                                    dominant_emotion = max(emotions.items(), key=lambda x: x[1])
                                    
                                    # Emoji dla emocji
                                    emotion_emoji = {
                                        'happy': 'üòä', 'sad': 'üò¢', 'angry': 'üò†', 'surprise': 'üòÆ', 
                                        'fear': 'üò®', 'disgust': 'ü§¢', 'neutral': 'üòê'
                                    }
                                    emoji = emotion_emoji.get(dominant_emotion[0], 'üé≠')
                                    
                                    # Aktualizuj wy≈õwietlanie
                                    with emotion_placeholder.container():
                                        st.markdown(f"""
                                        <div class="emotion-card">
                                            <h2>{emoji} {dominant_emotion[0].upper()}</h2>
                                            <h3>Pewno≈õƒá: {dominant_emotion[1]:.1f}%</h3>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    
                                    # Wy≈õwietl wszystkie emocje
                                    with stats_placeholder.container():
                                        for emotion, value in sorted(emotions.items(), key=lambda x: x[1], reverse=True)[:3]:
                                            emoji_e = emotion_emoji.get(emotion, 'üé≠')
                                            st.write(f"{emoji_e} {emotion}: {value:.1f}%")
                            except Exception:
                                pass
                    
                    time.sleep(0.5)  # Aktualizuj co p√≥≈Ç sekundy
        else:
            st.error("‚ö†Ô∏è Funkcjonalno≈õƒá kamery nie jest dostƒôpna w tym ≈õrodowisku.")
            st.info("üîÑ U≈ºyj opcji 'Przesy≈Çanie pliku' aby analizowaƒá emocje ze zdjƒôƒá.")
    
    uploaded_file = None  # Brak pliku dla kamery

if uploaded_file is not None:
    # Zapisz plik tymczasowo, bo DeepFace wymaga ≈õcie≈ºki do pliku
    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name
    
    # Sekcja wy≈õwietlania zdjƒôƒá
    st.markdown('<div class="sub-header">üñºÔ∏è Przes≈Çane Zdjƒôcie</div>', unsafe_allow_html=True)
    
    # Wy≈õwietl oryginalne zdjƒôcie w eleganckiej ramce
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.image(uploaded_file, caption="üì∑ Oryginalne zdjƒôcie", use_container_width=True)
    
    # Rozpocznij analizƒô
    st.markdown('<div class="sub-header">ü§ñ Analiza AI w Toku</div>', unsafe_allow_html=True)
    
    try:
        # Preload model if not already loaded
        load_deepface_model()
        
        # Clean memory before analysis
        cleanup_memory()
        
        # Analiza emocji u≈ºywajƒÖc DeepFace with better error handling
        with st.spinner('üîç Analizujƒô emocje i wykrywam twarz... To mo≈ºe potrwaƒá chwilƒô.'):
            try:
                result = DeepFace.analyze(
                    tmp_path, 
                    actions=['emotion'], 
                    enforce_detection=False,
                    silent=True,
                    detector_backend='opencv'  # Use more stable backend
                )
            except Exception as analysis_error:
                st.error(f"B≈ÇƒÖd podczas analizy obrazu: {str(analysis_error)}")
                st.info("Spr√≥buj u≈ºyƒá innego zdjƒôcia lub sprawd≈∫ czy twarz jest wyra≈∫nie widoczna.")
                raise analysis_error
        
        # Clean memory after analysis
        cleanup_memory()
        
        # Utw√≥rz wizualizacjƒô z zaznaczonƒÖ twarzƒÖ
        annotated_img, emotions, dominant_emotion = create_face_analysis_plot(tmp_path, result)
        
        if annotated_img is not None and emotions is not None and dominant_emotion is not None:
            # Sekcja wynik√≥w
            st.markdown('<div class="sub-header">üéØ Wyniki Analizy</div>', unsafe_allow_html=True)
            
            # Wy≈õwietl obraz z zaznaczonƒÖ twarzƒÖ i emocjƒÖ
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                st.image(annotated_img, caption=f"üé≠ Wykryta emocja: {dominant_emotion[0]} ({dominant_emotion[1]:.1f}%)", 
                        use_container_width=True)
            
            # Poka≈º dominujƒÖcƒÖ emocjƒô w eleganckiej karcie
            emotion_emoji = {
                'happy': 'üòä', 'sad': 'üò¢', 'angry': 'üò†', 'surprise': 'üòÆ', 
                'fear': 'üò®', 'disgust': 'ü§¢', 'neutral': 'üòê'
            }
            emotion_colors = {
                'happy': '#28a745', 'sad': '#6f42c1', 'angry': '#dc3545', 'surprise': '#ffc107', 
                'fear': '#6c757d', 'disgust': '#20c997', 'neutral': '#17a2b8'
            }
            
            emoji = emotion_emoji.get(dominant_emotion[0], 'üé≠')
            color = emotion_colors.get(dominant_emotion[0], '#17a2b8')
            
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, {color}22, {color}44); 
                        border-left: 4px solid {color}; 
                        padding: 1.5rem; 
                        border-radius: 10px; 
                        margin: 1rem 0;
                        text-align: center;">
                <h2>{emoji} DominujƒÖca Emocja: {dominant_emotion[0].upper()}</h2>
                <h3>Pewno≈õƒá: {dominant_emotion[1]:.1f}%</h3>
            </div>
            """, unsafe_allow_html=True)
        
        # Sekcja wykres√≥w i szczeg√≥≈Çowych analiz
        st.markdown('<div class="sub-header">üìä Szczeg√≥≈Çowa Analiza Emocji</div>', unsafe_allow_html=True)
        
        # Utw√≥rz dwie kolumny dla wykres√≥w
        col1, col2 = st.columns(2)
        
        with col1:
            # Wykres s≈Çupkowy emocji
            st.markdown("#### üìä Wykres s≈Çupkowy")
            if emotions is not None:
                fig, ax = plt.subplots(figsize=(8, 6))
                
                # Kolorowe s≈Çupki dla ka≈ºdej emocji
                emotion_colors_plot = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3', '#95e1d3']
                bars = ax.bar(list(emotions.keys()), list(emotions.values()), color=emotion_colors_plot[:len(emotions)])
                ax.set_ylabel('Pewno≈õƒá (%)', fontsize=12)
                ax.set_title('Rozk≈Çad wszystkich emocji', fontsize=14, pad=20)
                ax.set_ylim(0, 100)
                
                # Dodaj warto≈õci na s≈Çupkach
                for i, bar in enumerate(bars):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                           f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')
                
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)
        
        with col2:
            # Wykres ko≈Çowy emocji
            st.markdown("#### ü•ß Wykres ko≈Çowy")
            if emotions is not None:
                fig2, ax2 = plt.subplots(figsize=(8, 6))
                colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3', '#95e1d3']
                
                # Filtruj tylko emocje > 1% dla czytelno≈õci
                filtered_emotions = {k: v for k, v in emotions.items() if v > 1}
                if not filtered_emotions:  # Je≈õli wszystkie sƒÖ < 1%, poka≈º wszystkie
                    filtered_emotions = emotions
                
                pie_result = ax2.pie(
                    list(filtered_emotions.values()), 
                    labels=list(filtered_emotions.keys()), 
                    autopct='%1.1f%%', 
                    colors=colors[:len(filtered_emotions)], 
                    startangle=90,
                    textprops={'fontsize': 10}
                )
                # Handle variable unpacking based on return type
                if len(pie_result) == 3:
                    wedges, texts, autotexts = pie_result
                else:
                    wedges, texts = pie_result
                    autotexts = []
                ax2.set_title('Procentowy rozk≈Çad emocji', fontsize=14, pad=20)
                st.pyplot(fig2)
        
        # Szczeg√≥≈Çowa tabela wynik√≥w
        if emotions is not None:
            st.markdown('<div class="sub-header">üìã Ranking Emocji</div>', unsafe_allow_html=True)
            
            # Przygotuj dane do tabeli
            emotion_data = []
            for i, (emotion, value) in enumerate(sorted(emotions.items(), key=lambda x: x[1], reverse=True), 1):
                emoji_map = {
                    'happy': 'üòä', 'sad': 'üò¢', 'angry': 'üò†', 'surprise': 'üòÆ', 
                    'fear': 'üò®', 'disgust': 'ü§¢', 'neutral': 'üòê'
                }
                color_map = {
                    'happy': 'üü¢', 'sad': 'üîµ', 'angry': 'üî¥', 'surprise': 'üü°', 
                    'fear': '‚ö´', 'disgust': 'üü¢', 'neutral': '‚ö™'
                }
                
                emotion_data.append({
                    "Pozycja": f"{i}.",
                    "Emocja": f"{emoji_map.get(emotion, 'üé≠')} {emotion.title()}",
                    "Pewno≈õƒá": f"{value:.2f}%",
                    "Status": color_map.get(emotion, '‚ö™')
                })
            
            # Wy≈õwietl tabelƒô w 3 kolumnach
            col1, col2, col3 = st.columns([2, 1, 1])
            
            with col1:
                for item in emotion_data:
                    confidence = float(item["Pewno≈õƒá"].replace('%', ''))
                    if confidence > 50:
                        status_color = "üî• Wysoka"
                    elif confidence > 20:
                        status_color = "üî∂ ≈örednia"
                    else:
                        status_color = "üîπ Niska"
                    
                    st.markdown(f"""
                    <div class="emotion-card">
                        <strong>{item['Pozycja']} {item['Emocja']}</strong><br>
                        <span style="font-size: 1.2em; color: #1f77b4;">{item['Pewno≈õƒá']}</span>
                        <span style="float: right;">{status_color}</span>
                    </div>
                    """, unsafe_allow_html=True)
        
        else:
            st.markdown('<div class="sub-header">‚ùå Problem z AnalizƒÖ</div>', unsafe_allow_html=True)
            st.error("Nie uda≈Ço siƒô wykryƒá twarzy na zdjƒôciu.")
        
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas analizy: {str(e)}")
        st.info("Spr√≥buj u≈ºyƒá innego zdjƒôcia z wyra≈∫nie widocznƒÖ twarzƒÖ.")
    
    finally:
        # Usu≈Ñ plik tymczasowy
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)
        # Clean up memory after processing
        cleanup_memory()
