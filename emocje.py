import os
import sys

# Import Streamlit first to avoid conflicts
try:
    import streamlit as st
    st.write("ğŸš€ Starting Emotion Recognition App...")
    st.write("âœ… Streamlit imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import Streamlit: {e}")
    sys.exit(1)

# Set environment variables before any imports to handle Keras compatibility
os.environ['TF_USE_LEGACY_KERAS'] = '1'
os.environ['TF_KERAS'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['PYTHONHASHSEED'] = '0'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU usage to avoid GPU memory issues

# Memory management for stability
import gc
gc.set_threshold(700, 10, 10)

# Add error handling for imports
print("ğŸ”„ Starting application...")
print(f"Python version: {sys.version}")

try:
    print("ğŸ¤– Importing DeepFace...")
    from deepface import DeepFace
    print("âœ… DeepFace imported successfully")
    
    print("ğŸ“Š Importing matplotlib...")
    import matplotlib.pyplot as plt
    plt.switch_backend('Agg')  # Use non-interactive backend for stability
    print("âœ… matplotlib imported successfully")
    
    print("ğŸ‘ï¸ Importing OpenCV...")
    import cv2
    print("âœ… OpenCV imported successfully")
    
    print("ğŸ”¢ Importing numpy...")
    import numpy as np
    print("âœ… numpy imported successfully")
    import tempfile
    from PIL import Image, ImageDraw, ImageFont
    import matplotlib.patches as patches
    
    # Import with type stubs for optional webrtc
    try:
        from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration  # type: ignore
        import av
        WEBRTC_AVAILABLE = True
        
        # Define the real VideoProcessor when webrtc is available
        class VideoProcessor(VideoTransformerBase):  # type: ignore
            """Klasa do przetwarzania wideo z kamery w czasie rzeczywistym"""
            
            def __init__(self):
                self.frame_count = 0
                self.analyze_every_n_frames = 30  # Analizuj co 30 klatek (okoÅ‚o sekundy przy 30 FPS)
            
            def recv(self, frame):
                global latest_emotion_result, emotion_lock
                
                img = frame.to_ndarray(format="bgr24")
                
                # Analizuj emocje co N klatek Å¼eby nie obciÄ…Å¼aÄ‡ procesora
                if self.frame_count % self.analyze_every_n_frames == 0:
                    try:
                        # Zapisz klatkÄ™ tymczasowo
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                            cv2.imwrite(tmp_file.name, img)
                            
                            # Analizuj emocje (szybko, bez enforce_detection) with error handling
                            try:
                                result = DeepFace.analyze(
                                    tmp_file.name, 
                                    actions=['emotion'], 
                                    enforce_detection=False,
                                    silent=True,
                                    detector_backend='opencv'  # Use stable backend
                                )
                                
                                # Zapisz wynik
                                with emotion_lock:
                                    if isinstance(result, list):
                                        latest_emotion_result = result[0]
                                    else:
                                        latest_emotion_result = result
                            except Exception:
                                # Silently handle analysis errors in real-time mode
                                pass
                            
                            # UsuÅ„ plik tymczasowy
                            os.unlink(tmp_file.name)
                            
                    except Exception as e:
                        pass  # Zignoruj bÅ‚Ä™dy analizy
                
                # JeÅ›li mamy wynik analizy, narysuj na obrazie
                with emotion_lock:
                    if latest_emotion_result is not None:
                        try:
                            emotions = latest_emotion_result.get('emotion', {})  # type: ignore
                            face_region = latest_emotion_result.get('region', {})  # type: ignore
                            
                            if emotions and face_region:
                                # ZnajdÅº dominujÄ…cÄ… emocjÄ™
                                dominant_emotion = max(emotions.items(), key=lambda x: x[1])
                                
                                # Narysuj prostokÄ…t wokÃ³Å‚ twarzy
                                x, y, w, h = face_region.get('x', 0), face_region.get('y', 0), face_region.get('w', 0), face_region.get('h', 0)
                                if x > 0 and y > 0 and w > 0 and h > 0:
                                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
                                    
                                    # Dodaj tekst z emocjÄ…
                                    emotion_emoji = {
                                        'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 'surprise': 'ğŸ˜®', 
                                        'fear': 'ğŸ˜¨', 'disgust': 'ğŸ¤¢', 'neutral': 'ğŸ˜'
                                    }
                                    emoji = emotion_emoji.get(dominant_emotion[0], 'ğŸ­')
                                    label = f"{dominant_emotion[0]}: {dominant_emotion[1]:.1f}%"  # UsunÄ…Å‚em emoji bo moÅ¼e nie dziaÅ‚aÄ‡ w OpenCV
                                    
                                    # Rysuj tÅ‚o dla tekstu
                                    font = cv2.FONT_HERSHEY_SIMPLEX
                                    font_scale = 0.6
                                    thickness = 2
                                    (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
                                    
                                    cv2.rectangle(img, (x, y - text_height - 10), (x + text_width, y), (0, 255, 0), -1)
                                    cv2.putText(img, label, (x, y - 5), font, font_scale, (0, 0, 0), thickness)
                            
                        except Exception as e:
                            pass  # Zignoruj bÅ‚Ä™dy rysowania
                
                self.frame_count += 1
                return av.VideoFrame.from_ndarray(img, format="bgr24")
        
    except ImportError:
        WEBRTC_AVAILABLE = False
        st.warning("âš ï¸ Camera functionality is not available. Only file upload mode will work.")
        
        # Define dummy classes when webrtc is not available
        class VideoTransformerBase:
            pass
            
        class VideoProcessor:
            def __init__(self):
                self.frame_count = 0
                self.analyze_every_n_frames = 30
                
        class RTCConfiguration:
            def __init__(self, *args, **kwargs):
                pass
                
        def webrtc_streamer(*args, **kwargs):
            return None
    
    import threading
    import time
    from typing import Optional, Dict, Any, Tuple
except ImportError as e:
    print(f"Import error: {e}")
    sys.exit(1)

# Memory management functions
def cleanup_memory():
    """Clean up memory to prevent segmentation faults"""
    gc.collect()
    plt.close('all')

@st.cache_data(show_spinner=False)
def load_deepface_model():
    """Preload DeepFace model with caching"""
    try:
        # Trigger model loading with a dummy analysis
        import numpy as np
        dummy_img = np.zeros((224, 224, 3), dtype=np.uint8)
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
            cv2.imwrite(tmp_file.name, dummy_img)
            try:
                DeepFace.analyze(tmp_file.name, actions=['emotion'], enforce_detection=False, silent=True)
            except:
                pass
            finally:
                os.unlink(tmp_file.name)
        return True
    except Exception:
        return False

# Konfiguracja strony
st.set_page_config(
    page_title="ğŸ­ Analizator Emocji AI",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Stylizacja CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .sub-header {
        font-size: 1.5rem;
        color: #ff7f0e;
        margin: 1rem 0;
        border-left: 4px solid #ff7f0e;
        padding-left: 1rem;
    }
    .metric-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin: 0.5rem 0;
    }
    .emotion-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .sidebar-content {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    div.stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 0.5rem 2rem;
        font-weight: bold;
        transition: all 0.3s;
    }
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .uploadedFile {
        border: 2px dashed #1f77b4;
        border-radius: 10px;
        padding: 1rem;
        text-align: center;
        background: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)

def draw_emotion_on_face(image_path: str, face_region: Dict[str, int], dominant_emotion: str, confidence: float) -> Optional[np.ndarray]:
    """Rysuje prostokÄ…t wokÃ³Å‚ twarzy i oznacza emocjÄ™"""
    # Wczytaj obraz
    img = cv2.imread(image_path)
    if img is None:
        return None
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # Pobierz wspÃ³Å‚rzÄ™dne twarzy
    x, y, w, h = face_region['x'], face_region['y'], face_region['w'], face_region['h']
    
    # Narysuj prostokÄ…t wokÃ³Å‚ twarzy
    cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 0), 3)
    
    # Dodaj tekst z emocjÄ…
    label = f"{dominant_emotion}: {confidence:.1f}%"
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.8
    thickness = 2
    
    # Oblicz rozmiar tekstu
    (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
    
    # Narysuj tÅ‚o dla tekstu
    cv2.rectangle(img_rgb, (x, y - text_height - 10), (x + text_width, y), (0, 255, 0), -1)
    
    # Narysuj tekst
    cv2.putText(img_rgb, label, (x, y - 5), font, font_scale, (0, 0, 0), thickness)
    
    return img_rgb

def create_face_analysis_plot(image_path: str, result: Any) -> Tuple[Optional[np.ndarray], Optional[Dict[str, float]], Optional[Tuple[str, float]]]:
    """Tworzy wykres z zaznaczonÄ… twarzÄ… i emocjami"""
    # Wczytaj obraz
    img = cv2.imread(image_path)
    if img is None:
        return None, None, None
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # Przygotuj dane
    if isinstance(result, list):
        face_data = result[0]
    else:
        face_data = result
    
    emotions = face_data['emotion']
    face_region = face_data['region']
    dominant_emotion = max(emotions.items(), key=lambda x: x[1])
    
    # Narysuj obraz z oznaczonÄ… twarzÄ…
    annotated_img = draw_emotion_on_face(image_path, face_region, dominant_emotion[0], dominant_emotion[1])
    
    return annotated_img, emotions, dominant_emotion

# Globalne zmienne dla kamery
latest_emotion_result = None
emotion_lock = threading.Lock()

# GÅ‚Ã³wny nagÅ‚Ã³wek aplikacji
st.markdown('<h1 class="main-header">ğŸ­ Analizator Emocji AI</h1>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Wykrywaj emocje na zdjÄ™ciach dziÄ™ki sztucznej inteligencji</p>', unsafe_allow_html=True)

# === SIDEBAR ===
st.sidebar.markdown("## ï¿½ï¸ Panel Kontrolny")

# Informacje o aplikacji
with st.sidebar.expander("â„¹ï¸ O Aplikacji", expanded=True):
    st.markdown("""
    **ï¿½ğŸ­ Analizator Emocji AI** uÅ¼ywa zaawansowanych algorytmÃ³w deep learning do:
    
    â€¢ ğŸ” Wykrywania twarzy na zdjÄ™ciach
    â€¢ ğŸ¯ Analizy wyrazu emocjonalnego
    â€¢ ğŸ“Š Generowania szczegÃ³Å‚owych raportÃ³w
    â€¢ ğŸ¨ Wizualizacji wynikÃ³w
    """)

# Ustawienia analizy
st.sidebar.markdown("### âš™ï¸ Ustawienia Analizy")

# WybÃ³r ÅºrÃ³dÅ‚a obrazu
if WEBRTC_AVAILABLE:
    source_options = ["ğŸ“¸ PrzesyÅ‚anie pliku", "ğŸ“¹ Kamera internetowa"]
else:
    source_options = ["ğŸ“¸ PrzesyÅ‚anie pliku"]
    
source_option = st.sidebar.radio(
    "ğŸ“¹ Å¹rÃ³dÅ‚o obrazu:",
    source_options,
    help="Wybierz skÄ…d chcesz analizowaÄ‡ emocje"
)

confidence_threshold = st.sidebar.slider(
    "ğŸ¯ PrÃ³g pewnoÅ›ci (%)", 
    min_value=0, 
    max_value=100, 
    value=50,
    help="Minimalny poziom pewnoÅ›ci dla wykrywania twarzy"
)

show_advanced = st.sidebar.checkbox("ğŸ”¬ PokaÅ¼ zaawansowane opcje", False)

if show_advanced:
    detection_backend = st.sidebar.selectbox(
        "ğŸ” Backend wykrywania",
        ["opencv", "retinaface", "mtcnn"],
        index=0,
        help="Wybierz algorytm wykrywania twarzy"
    )
    
    model_name = st.sidebar.selectbox(
        "ğŸ§  Model analizy",
        ["VGG-Face", "Facenet", "OpenFace", "DeepFace"],
        index=0,
        help="Wybierz model do analizy emocji"
    )
else:
    detection_backend = "opencv"
    model_name = "VGG-Face"

# Statystyki
st.sidebar.markdown("### ğŸ“ˆ Statystyki")
with st.sidebar.container():
    st.markdown("""
    <div class="metric-container">
        <h4>ğŸ¯ Emocje do wykrycia:</h4>
        <p>ğŸ˜Š RadoÅ›Ä‡ â€¢ ğŸ˜¢ Smutek â€¢ ğŸ˜  ZÅ‚oÅ›Ä‡<br>
        ğŸ˜® Zdziwienie â€¢ ğŸ˜¨ Strach â€¢ ğŸ¤¢ Obrzydzenie<br>
        ğŸ˜ NeutralnoÅ›Ä‡</p>
    </div>
    """, unsafe_allow_html=True)

# Pomoc
with st.sidebar.expander("ğŸ’¡ WskazÃ³wki"):
    st.markdown("""
    **ğŸ“¸ Najlepsze rezultaty osiÄ…gniesz gdy:**
    
    âœ… Twarz jest dobrze oÅ›wietlona  
    âœ… Osoba patrzy w kierunku kamery  
    âœ… ZdjÄ™cie jest ostre i wyraÅºne  
    âœ… Twarz zajmuje wiÄ™kszÄ… czÄ™Å›Ä‡ kadru  
    
    **âŒ Unikaj:**
    
    âŒ ZdjÄ™Ä‡ zbyt ciemnych lub jasnych  
    âŒ Twarzy zakrytych lub w profilu  
    âŒ ZdjÄ™Ä‡ rozmytych lub pixelowanych  
    """)

# === GÅÃ“WNA ZAWARTOÅšÄ† ===

if source_option == "ğŸ“¸ PrzesyÅ‚anie pliku":
    # Sekcja upload-u zdjÄ™cia
    st.markdown('<div class="sub-header">ğŸ“¸ PrzeÅ›lij ZdjÄ™cie do Analizy</div>', unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        uploaded_file = st.file_uploader(
            "",
            type=["jpg", "jpeg", "png", "bmp", "tiff"],
            help="Wybierz zdjÄ™cie z wyraÅºnie widocznÄ… twarzÄ…",
            label_visibility="collapsed"
        )
        
        if uploaded_file is None:
            st.markdown("""
            <div class="uploadedFile">
                <h3>ğŸ“¸ PrzeciÄ…gnij i upuÅ›Ä‡ zdjÄ™cie tutaj</h3>
                <p>lub kliknij aby wybraÄ‡ plik</p>
                <p><small>ObsÅ‚ugiwane formaty: JPG, PNG, BMP, TIFF</small></p>
            </div>
            """, unsafe_allow_html=True)

else:  # Kamera internetowa
    if WEBRTC_AVAILABLE:
        st.markdown('<div class="sub-header">ğŸ“¹ Kamera Internetowa - Analiza Real-time</div>', unsafe_allow_html=True)
        
        st.markdown("""
        <div class="emotion-card">
            <h4>ğŸ¥ Analiza emocji w czasie rzeczywistym</h4>
            <p>â€¢ Kamera analizuje Twoje emocje na Å¼ywo</p>
            <p>â€¢ Wyniki sÄ… aktualizowane co okoÅ‚o sekundÄ™</p>
            <p>â€¢ Zielony prostokÄ…t pokazuje wykrytÄ… twarz</p>
            <p>â€¢ Nazwa emocji pojawia siÄ™ nad twarzÄ…</p>
        </div>
        """, unsafe_allow_html=True)
        
        if WEBRTC_AVAILABLE:
            # Konfiguracja WebRTC
            RTC_CONFIGURATION = RTCConfiguration({
                "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
            })
            
            # Stream z kamery z analizÄ… emocji
            webrtc_ctx = webrtc_streamer(
                key="emotion-analysis",
                video_processor_factory=VideoProcessor,
                rtc_configuration=RTC_CONFIGURATION,
                media_stream_constraints={"video": True, "audio": False},
                async_processing=True,
            )
            
            # WyÅ›wietlaj bieÅ¼Ä…ce wyniki analizy
            if webrtc_ctx and webrtc_ctx.video_processor:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ğŸ“Š BieÅ¼Ä…ca Analiza")
                    emotion_placeholder = st.empty()
                    
                with col2:
                    st.markdown("#### ğŸ¯ Statystyki")
                    stats_placeholder = st.empty()
                
                # Aktualizuj wyniki w czasie rzeczywistym
                if hasattr(webrtc_ctx, 'state') and webrtc_ctx.state.playing:
                    with emotion_lock:
                        if latest_emotion_result is not None:
                            try:
                                emotions = latest_emotion_result.get('emotion', {})  # type: ignore
                                if emotions:
                                    dominant_emotion = max(emotions.items(), key=lambda x: x[1])
                                    
                                    # Emoji dla emocji
                                    emotion_emoji = {
                                        'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 'surprise': 'ğŸ˜®', 
                                        'fear': 'ğŸ˜¨', 'disgust': 'ğŸ¤¢', 'neutral': 'ğŸ˜'
                                    }
                                    emoji = emotion_emoji.get(dominant_emotion[0], 'ğŸ­')
                                    
                                    # Aktualizuj wyÅ›wietlanie
                                    with emotion_placeholder.container():
                                        st.markdown(f"""
                                        <div class="emotion-card">
                                            <h2>{emoji} {dominant_emotion[0].upper()}</h2>
                                            <h3>PewnoÅ›Ä‡: {dominant_emotion[1]:.1f}%</h3>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    
                                    # WyÅ›wietl wszystkie emocje
                                    with stats_placeholder.container():
                                        for emotion, value in sorted(emotions.items(), key=lambda x: x[1], reverse=True)[:3]:
                                            emoji_e = emotion_emoji.get(emotion, 'ğŸ­')
                                            st.write(f"{emoji_e} {emotion}: {value:.1f}%")
                            except Exception:
                                pass
                    
                    time.sleep(0.5)  # Aktualizuj co pÃ³Å‚ sekundy
        else:
            st.error("âš ï¸ FunkcjonalnoÅ›Ä‡ kamery nie jest dostÄ™pna w tym Å›rodowisku.")
            st.info("ğŸ”„ UÅ¼yj opcji 'PrzesyÅ‚anie pliku' aby analizowaÄ‡ emocje ze zdjÄ™Ä‡.")
    
    uploaded_file = None  # Brak pliku dla kamery

if uploaded_file is not None:
    # Zapisz plik tymczasowo, bo DeepFace wymaga Å›cieÅ¼ki do pliku
    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name
    
    # Sekcja wyÅ›wietlania zdjÄ™Ä‡
    st.markdown('<div class="sub-header">ğŸ–¼ï¸ PrzesÅ‚ane ZdjÄ™cie</div>', unsafe_allow_html=True)
    
    # WyÅ›wietl oryginalne zdjÄ™cie w eleganckiej ramce
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.image(uploaded_file, caption="ğŸ“· Oryginalne zdjÄ™cie", use_container_width=True)
    
    # Rozpocznij analizÄ™
    st.markdown('<div class="sub-header">ğŸ¤– Analiza AI w Toku</div>', unsafe_allow_html=True)
    
    try:
        # Preload model if not already loaded
        load_deepface_model()
        
        # Clean memory before analysis
        cleanup_memory()
        
        # Analiza emocji uÅ¼ywajÄ…c DeepFace with better error handling
        with st.spinner('ğŸ” AnalizujÄ™ emocje i wykrywam twarz... To moÅ¼e potrwaÄ‡ chwilÄ™.'):
            try:
                result = DeepFace.analyze(
                    tmp_path, 
                    actions=['emotion'], 
                    enforce_detection=False,
                    silent=True,
                    detector_backend='opencv'  # Use more stable backend
                )
            except Exception as analysis_error:
                st.error(f"BÅ‚Ä…d podczas analizy obrazu: {str(analysis_error)}")
                st.info("SprÃ³buj uÅ¼yÄ‡ innego zdjÄ™cia lub sprawdÅº czy twarz jest wyraÅºnie widoczna.")
                raise analysis_error
        
        # Clean memory after analysis
        cleanup_memory()
        
        # UtwÃ³rz wizualizacjÄ™ z zaznaczonÄ… twarzÄ…
        annotated_img, emotions, dominant_emotion = create_face_analysis_plot(tmp_path, result)
        
        if annotated_img is not None and emotions is not None and dominant_emotion is not None:
            # Sekcja wynikÃ³w
            st.markdown('<div class="sub-header">ğŸ¯ Wyniki Analizy</div>', unsafe_allow_html=True)
            
            # WyÅ›wietl obraz z zaznaczonÄ… twarzÄ… i emocjÄ…
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                st.image(annotated_img, caption=f"ğŸ­ Wykryta emocja: {dominant_emotion[0]} ({dominant_emotion[1]:.1f}%)", 
                        use_container_width=True)
            
            # PokaÅ¼ dominujÄ…cÄ… emocjÄ™ w eleganckiej karcie
            emotion_emoji = {
                'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 'surprise': 'ğŸ˜®', 
                'fear': 'ğŸ˜¨', 'disgust': 'ğŸ¤¢', 'neutral': 'ğŸ˜'
            }
            emotion_colors = {
                'happy': '#28a745', 'sad': '#6f42c1', 'angry': '#dc3545', 'surprise': '#ffc107', 
                'fear': '#6c757d', 'disgust': '#20c997', 'neutral': '#17a2b8'
            }
            
            emoji = emotion_emoji.get(dominant_emotion[0], 'ğŸ­')
            color = emotion_colors.get(dominant_emotion[0], '#17a2b8')
            
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, {color}22, {color}44); 
                        border-left: 4px solid {color}; 
                        padding: 1.5rem; 
                        border-radius: 10px; 
                        margin: 1rem 0;
                        text-align: center;">
                <h2>{emoji} DominujÄ…ca Emocja: {dominant_emotion[0].upper()}</h2>
                <h3>PewnoÅ›Ä‡: {dominant_emotion[1]:.1f}%</h3>
            </div>
            """, unsafe_allow_html=True)
        
        # Sekcja wykresÃ³w i szczegÃ³Å‚owych analiz
        st.markdown('<div class="sub-header">ğŸ“Š SzczegÃ³Å‚owa Analiza Emocji</div>', unsafe_allow_html=True)
        
        # UtwÃ³rz dwie kolumny dla wykresÃ³w
        col1, col2 = st.columns(2)
        
        with col1:
            # Wykres sÅ‚upkowy emocji
            st.markdown("#### ğŸ“Š Wykres sÅ‚upkowy")
            if emotions is not None:
                fig, ax = plt.subplots(figsize=(8, 6))
                
                # Kolorowe sÅ‚upki dla kaÅ¼dej emocji
                emotion_colors_plot = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3', '#95e1d3']
                bars = ax.bar(list(emotions.keys()), list(emotions.values()), color=emotion_colors_plot[:len(emotions)])
                ax.set_ylabel('PewnoÅ›Ä‡ (%)', fontsize=12)
                ax.set_title('RozkÅ‚ad wszystkich emocji', fontsize=14, pad=20)
                ax.set_ylim(0, 100)
                
                # Dodaj wartoÅ›ci na sÅ‚upkach
                for i, bar in enumerate(bars):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                           f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')
                
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)
        
        with col2:
            # Wykres koÅ‚owy emocji
            st.markdown("#### ğŸ¥§ Wykres koÅ‚owy")
            if emotions is not None:
                fig2, ax2 = plt.subplots(figsize=(8, 6))
                colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3', '#95e1d3']
                
                # Filtruj tylko emocje > 1% dla czytelnoÅ›ci
                filtered_emotions = {k: v for k, v in emotions.items() if v > 1}
                if not filtered_emotions:  # JeÅ›li wszystkie sÄ… < 1%, pokaÅ¼ wszystkie
                    filtered_emotions = emotions
                
                pie_result = ax2.pie(
                    list(filtered_emotions.values()), 
                    labels=list(filtered_emotions.keys()), 
                    autopct='%1.1f%%', 
                    colors=colors[:len(filtered_emotions)], 
                    startangle=90,
                    textprops={'fontsize': 10}
                )
                # Handle variable unpacking based on return type
                if len(pie_result) == 3:
                    wedges, texts, autotexts = pie_result
                else:
                    wedges, texts = pie_result
                    autotexts = []
                ax2.set_title('Procentowy rozkÅ‚ad emocji', fontsize=14, pad=20)
                st.pyplot(fig2)
        
        # SzczegÃ³Å‚owa tabela wynikÃ³w
        if emotions is not None:
            st.markdown('<div class="sub-header">ğŸ“‹ Ranking Emocji</div>', unsafe_allow_html=True)
            
            # Przygotuj dane do tabeli
            emotion_data = []
            for i, (emotion, value) in enumerate(sorted(emotions.items(), key=lambda x: x[1], reverse=True), 1):
                emoji_map = {
                    'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 'surprise': 'ğŸ˜®', 
                    'fear': 'ğŸ˜¨', 'disgust': 'ğŸ¤¢', 'neutral': 'ğŸ˜'
                }
                color_map = {
                    'happy': 'ğŸŸ¢', 'sad': 'ğŸ”µ', 'angry': 'ğŸ”´', 'surprise': 'ğŸŸ¡', 
                    'fear': 'âš«', 'disgust': 'ğŸŸ¢', 'neutral': 'âšª'
                }
                
                emotion_data.append({
                    "Pozycja": f"{i}.",
                    "Emocja": f"{emoji_map.get(emotion, 'ğŸ­')} {emotion.title()}",
                    "PewnoÅ›Ä‡": f"{value:.2f}%",
                    "Status": color_map.get(emotion, 'âšª')
                })
            
            # WyÅ›wietl tabelÄ™ w 3 kolumnach
            col1, col2, col3 = st.columns([2, 1, 1])
            
            with col1:
                for item in emotion_data:
                    confidence = float(item["PewnoÅ›Ä‡"].replace('%', ''))
                    if confidence > 50:
                        status_color = "ğŸ”¥ Wysoka"
                    elif confidence > 20:
                        status_color = "ğŸ”¶ Åšrednia"
                    else:
                        status_color = "ğŸ”¹ Niska"
                    
                    st.markdown(f"""
                    <div class="emotion-card">
                        <strong>{item['Pozycja']} {item['Emocja']}</strong><br>
                        <span style="font-size: 1.2em; color: #1f77b4;">{item['PewnoÅ›Ä‡']}</span>
                        <span style="float: right;">{status_color}</span>
                    </div>
                    """, unsafe_allow_html=True)
        
        else:
            st.markdown('<div class="sub-header">âŒ Problem z AnalizÄ…</div>', unsafe_allow_html=True)
            st.error("Nie udaÅ‚o siÄ™ wykryÄ‡ twarzy na zdjÄ™ciu.")
        
    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas analizy: {str(e)}")
        st.info("SprÃ³buj uÅ¼yÄ‡ innego zdjÄ™cia z wyraÅºnie widocznÄ… twarzÄ….")
    
    finally:
        # UsuÅ„ plik tymczasowy
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)
        # Clean up memory after processing
        cleanup_memory()
